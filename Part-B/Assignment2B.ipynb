{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms, datasets, models\nfrom torch.utils.data import DataLoader, Subset\nimport wandb\n\n# ---- Hyperparameters ----\nh_params = {\n    \"epochs\": 15,\n    \"learning_rate\": 0.0005,\n    \"batch_size\": 32,\n    \"last_unfreeze_layers\": 3,  # Unfreeze more classifier layers\n    \"image_size\": 224,\n    \"num_classes\": 10,\n    \"num_workers\": 2,\n    \"seed\": 42\n}\n\nIMAGE_SIZE = h_params[\"image_size\"]\nNUM_OF_CLASSES = h_params[\"num_classes\"]\n\n# ---- Set random seed for reproducibility ----\ntorch.manual_seed(h_params[\"seed\"])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- Data Preparation Class ----\nclass DataPreparer:\n    def __init__(self, h_params, image_size, train_dir, val_dir):\n        self.h_params = h_params\n        self.image_size = image_size\n        self.train_dir = train_dir\n        self.val_dir = val_dir\n\n    def get_train_transform(self):\n        size = (self.image_size, self.image_size)\n        return transforms.Compose([\n            transforms.Resize(size),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n            transforms.GaussianBlur(kernel_size=3),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    def get_test_transform(self):\n        size = (self.image_size, self.image_size)\n        return transforms.Compose([\n            transforms.Resize(size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n\n    def stratified_split(self, dataset, ratio):\n        train_idx, val_idx = [], []\n        class_bounds = [\n            (0, 999), (1000, 1999), (2000, 2999), (3000, 3999), (4000, 4998),\n            (4999, 5998), (5999, 6998), (6999, 7998), (7999, 8998), (8999, 9998)\n        ]\n        for start, end in class_bounds:\n            indices = list(range(start, end + 1))\n            split_at = int(len(indices) * ratio)\n            train_idx.extend(indices[:split_at])\n            val_idx.extend(indices[split_at:])\n        return Subset(dataset, train_idx), Subset(dataset, val_idx)\n\n    def get_datasets(self):\n        train_transform = self.get_train_transform()\n        test_transform = self.get_test_transform()\n        full_train = datasets.ImageFolder(self.train_dir, transform=train_transform)\n        train_set, val_set = self.stratified_split(full_train, 0.8)\n        test_set = datasets.ImageFolder(self.val_dir, transform=test_transform)\n        return train_set, val_set, test_set\n\n    def get_loaders(self):\n        train_set, val_set, test_set = self.get_datasets()\n        batch = self.h_params[\"batch_size\"]\n        return {\n            \"train_loader\": DataLoader(train_set, batch_size=batch, shuffle=True, num_workers=self.h_params[\"num_workers\"]),\n            \"val_loader\": DataLoader(val_set, batch_size=batch, shuffle=False, num_workers=self.h_params[\"num_workers\"]),\n            \"test_loader\": DataLoader(test_set, batch_size=batch, shuffle=False, num_workers=self.h_params[\"num_workers\"]),\n            \"train_len\": len(train_set),\n            \"val_len\": len(val_set),\n            \"test_len\": len(test_set)\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def efficientnetv2_model(h_params):\n    model = models.efficientnet_v2_s(weights=\"IMAGENET1K_V1\")\n    num_ftrs = model.classifier[1].in_features\n    model.classifier[1] = nn.Linear(num_ftrs, NUM_OF_CLASSES)\n\n    # Freeze all layers\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Unfreeze the last k layers of the classifier explicitly\n    k = h_params[\"last_unfreeze_layers\"]\n    if k > 0:\n        classifier_layers = list(model.classifier.children())\n        # Unfreeze parameters in the last k layers of the classifier\n        for layer in classifier_layers[-k:]:\n            for param in layer.parameters():\n                param.requires_grad = True\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- Trainer Class ----\nclass Trainer:\n    def __init__(self, model_class, h_params, training_data):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model = model_class(h_params)\n        self.model = torch.nn.DataParallel(self.model).to(self.device)\n        self.h_params = h_params\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), lr=h_params[\"learning_rate\"])\n        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=5, gamma=0.5)\n        self.train_loader = training_data['train_loader']\n        self.val_loader = training_data['val_loader']\n        self.test_loader = training_data['test_loader']\n        self.train_len = training_data['train_len']\n        self.val_len = training_data['val_len']\n        self.test_len = training_data['test_len']\n\n    def fit(self):\n        for epoch in range(self.h_params[\"epochs\"]):\n            train_loss, train_acc = self._train_one_epoch(epoch)\n            val_loss, val_acc = self._validate(epoch)\n            print(f\"epoch: {epoch} train accuracy: {train_acc:.4f} train loss: {train_loss:.4f} \"\n                  f\"val accuracy: {val_acc:.4f} val loss: {val_loss:.4f}\")\n            wandb.log({\n                \"train_accuracy\": train_acc,\n                \"train_loss\": train_loss,\n                \"val_accuracy\": val_acc,\n                \"val_loss\": val_loss,\n                \"epoch\": epoch\n            })\n            self.scheduler.step()\n        test_loss, test_acc = self._test()\n        wandb.log({\n            \"test_accuracy\": test_acc,\n            \"test_loss\": test_loss\n        })\n        print(f'Test accuracy: {test_acc}, Test loss: {test_loss}')\n        print('Finished Training')\n        torch.save(self.model.state_dict(), './bestmodel.pth')\n\n    def _train_one_epoch(self, epoch):\n        self.model.train()\n        running_loss = 0.0\n        correct = 0\n        for i, (inputs, labels) in enumerate(self.train_loader):\n            inputs, labels = inputs.to(self.device), labels.to(self.device)\n            self.optimizer.zero_grad()\n            outputs = self.model(inputs)\n            loss = self.loss_fn(outputs, labels)\n            loss.backward()\n            self.optimizer.step()\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            if i % 10 == 0:\n                batch_acc = (predicted == labels).float().mean().item()\n                print(f\"epoch {epoch} batch {i} accuracy {batch_acc:.4f} loss {loss.item():.4f}\")\n        avg_loss = running_loss / len(self.train_loader)\n        accuracy = correct / self.train_len\n        return avg_loss, accuracy\n\n    def _validate(self, epoch):\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        with torch.no_grad():\n            for inputs, labels in self.val_loader:\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\n                outputs = self.model(inputs)\n                loss = self.loss_fn(outputs, labels)\n                running_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                correct += (predicted == labels).sum().item()\n        avg_loss = running_loss / len(self.val_loader)\n        accuracy = correct / self.val_len\n        return avg_loss, accuracy\n\n    def _test(self):\n        self.model.eval()\n        running_loss = 0.0\n        correct = 0\n        with torch.no_grad():\n            for inputs, labels in self.test_loader:\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\n                outputs = self.model(inputs)\n                loss = self.loss_fn(outputs, labels)\n                running_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                correct += (predicted == labels).sum().item()\n        avg_loss = running_loss / len(self.test_loader)\n        accuracy = correct / self.test_len\n        return avg_loss, accuracy\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ---- Main Training Run ----\nwandb.login(key=\"c4c7a78b7e8600d02ded519f43e6ef09838dc431\")  # Place your wandb key here\n\ntrain_data_dir = \"/kaggle/input/assignment2dataset/inaturalist_12K/train\"\ntest_data_dir = \"/kaggle/input/assignment2dataset/inaturalist_12K/val\"\ndata_preparer = DataPreparer(h_params, IMAGE_SIZE, train_data_dir, test_data_dir)\ntraining_data = data_preparer.get_loaders()\n\nrun = wandb.init(\n    project=\"DL Assignment 2B\",\n    name=f\"efficientnetv2_ep_{h_params['epochs']}_bs_{h_params['batch_size']}_lr_{h_params['learning_rate']}_last_unfreeze_layers_{h_params['last_unfreeze_layers']}\",\n    config=h_params\n)\n\ntrainer = Trainer(efficientnetv2_model, h_params, training_data)\ntrainer.fit()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.login(key=\"c4c7a78b7e8600d02ded519f43e6ef09838dc431\")  # Place your wandb key here\n# --- Sweep Configuration for EfficientNetV2-S (with learning_rate as hyperparameter) ---\nsweep_config = {\n    'method': 'bayes',\n    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'last_unfreeze_layers': {'values': [1, 3, 5]},\n        'dropout': {'values': [0, 0.2, 0.4]},\n        'batch_size': {'values': [32, 64, 128]},\n        'epochs': {'values': [5, 10]},\n        'learning_rate': {'values': [0.0001, 0.0005, 0.001, 0.005]}\n    }\n}\n\n# Create the sweep\nsweep_id = wandb.sweep(sweep=sweep_config, project=\"DL Assignment 2B\")\n\ndef sweep_train():\n    wandb.init()\n    config = wandb.config\n\n    # Set up h_params for this run\n    h_params = {\n        \"epochs\": config.epochs,\n        \"learning_rate\": config.learning_rate,\n        \"batch_size\": config.batch_size,\n        \"last_unfreeze_layers\": config.last_unfreeze_layers,\n        \"image_size\": 224,\n        \"num_classes\": 10,\n        \"num_workers\": 2,\n        \"seed\": 42,\n        \"dropout\": config.dropout\n    }\n\n    torch.manual_seed(h_params[\"seed\"])\n\n    train_data_dir = \"/kaggle/input/assignment2dataset/inaturalist_12K/train\"\n    test_data_dir = \"/kaggle/input/assignment2dataset/inaturalist_12K/val\"\n\n    # DataPreparer, efficientnetv2_model, and Trainer classes are already defined in previous cells\n\n    data_preparer = DataPreparer(h_params, h_params[\"image_size\"], train_data_dir, test_data_dir)\n    training_data = data_preparer.get_loaders()\n\n    run = wandb.init(\n        project=\"DL Assignment 2B\",\n        config=h_params\n    )\n\n    trainer = Trainer(efficientnetv2_model, h_params, training_data)\n    trainer.fit()\n\n# Launch the sweep agent (runs 10 trials)\nwandb.agent(sweep_id, function=sweep_train, count=10)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-19T22:07:25.750Z"}},"outputs":[],"execution_count":null}]}